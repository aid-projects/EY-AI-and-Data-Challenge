{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f28345fb-d896-453d-a825-2bdd518ba2a4",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "# Feature Selection \n\nThis notebook functions as a feature selector for each of the three response variables. To use this: \n\n1. Feed in a dataframe with all of the features that would like to be tested\n2. Run the code in this Notebook\n\nThis should return the features for each of the three response variables that are the best predictors.",
      "execution_count": null
    },
    {
      "id": "9533a38b-77ab-4741-ba18-aec16e64aca5",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Notes and Literature:\n\nLasso is generally better than most other methods for feature fitting. It will return a list of features, and the remainder it 'shrinks' to 0, as such can avoids overfitting - and tests better on out of sample data (prediction). The downside is that it assumes a linear relationship between the variables (variables can be transformed, but after transformation it produces a linear fit). It can be fine tuned with\n 1. Choice of Alpha (higher alpha gives more penalization, shrinking more features to 0)\n 2. Transformation of response variable (GLMs)\n\nRandom Forest is a non-linear approach to the data, but will not give a set of features. It does list the features based on importance, cut offs are based on other rules that need to be applied by the user. Tuning is given by:\n 1. n_estimators, which is the number of estimators in the tree (Although this is moreso a tradeoff on performance not model tuning)\n 2. Where to cut features off\n\nDifferent choices can give different results, so for each set of features, pick the tuning that gives the model w.r.t the chosen criteria"
    },
    {
      "id": "7efc3b8c-7761-4079-82cc-c2ac1b74a9f0",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Preparation\n\nJust running through our feature selection now\n\n### Loading packages / libraries"
    },
    {
      "id": "f1cbbd84-acac-4721-8a21-bf04593f6f4d",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Installing requirements",
        "title": "Installing requirements"
      },
      "source": "!pip install uv\n!uv pip install  -r requirements.txt \n\n#new library\n!pip install mlxtend\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7c24ed8b-7a47-4d65-bc0f-a0df75b99b8f",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Restart Kernel here\n\nThen we load in packages"
    },
    {
      "id": "fa0d7c03-169d-47c9-ba10-97365a11f47c",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Install Packages",
        "title": "Install Packages"
      },
      "source": "## import packages\n\nimport snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\n# Geospatial raster data handling with CRS support\nimport rioxarray as rxr\n\n# Raster operations and spatial windowing\nimport rasterio\nfrom rasterio.windows import Window\n\n# Feature preprocessing and data splitting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom scipy.spatial import cKDTree\n\n# Machine Learning\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\nfrom odc.stac import stac_load\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os \n\n#NEW PACKAGES\nimport planetary_computer \nimport dask \nfrom scipy import stats\nfrom datetime import datetime\nfrom dask.distributed import Client\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LassoCV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9378bd87-7c7d-462c-a68d-88fdea3ac732",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Loading in Data\n\nLoading in dataframe for model selection\n\nN.B when applying custom dataset, load it in as wq_data below. This has some transformations to scrap junk data and remove all nulls. This may or may not be requried in other functions"
    },
    {
      "id": "9e6e9679-71ba-4e01-9eb1-e94ad43e8592",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Loading in all data",
        "title": "Loading in all data"
      },
      "source": "Water_Quality_df = pd.read_csv(\"water_quality_training_dataset.csv\")\nlandsat_train_features = pd.read_csv(\"landsat_features_training_all_bands.csv\")\nTerraclimate_df = pd.read_csv(\"terraclimate_features_training.csv\")\n\n#landsat_train_features['NDMI'] = landsat_train_features['NDMI'].astype(float)\n#landsat_train_features['MNDWI'] = landsat_train_features['MNDWI'].astype(float)\nlandsat_train_features['Sample Date'] = pd.to_datetime(landsat_train_features['Sample Date'],  format='%d-%m-%Y')\n\ndef combine_two_datasets(dataset1,dataset2,dataset3):\n    '''\n    Returns a  vertically concatenated dataset.\n    Attributes:\n    dataset1 - Dataset 1 to be combined \n    dataset2 - Dataset 2 to be combined\n    '''\n    \n    data = pd.concat([dataset1,dataset2,dataset3], axis=1)\n    data = data.loc[:, ~data.columns.duplicated()]\n    return data\n\nwq_data = combine_two_datasets(Water_Quality_df, landsat_train_features, Terraclimate_df)\nwq_data['Sample Date'] = pd.to_datetime(wq_data['Sample Date'],  format='%d-%m-%Y')\nwq_data = wq_data.drop(columns=[\"Unnamed: 0\"])\n\n#ullify all negative observations\nfor column in wq_data.columns:\n    if column != \"Sample Date\": wq_data[wq_data[column] < -0.1][column] = np.nan \n\nwq_data = wq_data.drop(columns=['qa_radsat', 'cloud_qa', 'Sample Date'])\nwq_data = wq_data.dropna(how='any',axis=0)\n\nwq_data.info()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "554a1076-6c6c-490f-a0ef-31a4f1cc09eb",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "This next section is to apply any transformations to the dataset. For example we take the box-cox transformation of the Y data. And for the predictor lwir.1 we take the square. \n\nBut this transformation is arbitrary. Fit any transformations here\n\n"
    },
    {
      "id": "9d8791b5-3ae9-481f-b80e-4419597b3e2f",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Transformation of Variables",
        "title": "Transformation of Variables"
      },
      "source": "#Box Cox of predictors\nTotal_Alkalinity_bc, Total_Alkalinity_lambda_opt = stats.boxcox(wq_data['Total Alkalinity'])\nElectrical_Conductance_bc, Electrical_Conductance_lambda_opt = stats.boxcox(wq_data['Electrical Conductance'])\nDissolved_Reactive_Phosphorus_bc, Dissolved_Reactive_Phosphorus_lambda_opt = stats.boxcox(wq_data['Dissolved Reactive Phosphorus'])\n\nwq_data['Total Alkalinity'] = Total_Alkalinity_bc\nwq_data['Electrical Conductance'] = Electrical_Conductance_bc\nwq_data['Dissolved Reactive Phosphorus'] = Dissolved_Reactive_Phosphorus_bc\n\n#Square x value\nsquarelwir1 = wq_data['lwir.1'] ** 2\nwq_data['lwir.1'] = squarelwir1",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8aa213cb-d4f3-4cb8-bd92-b52447b2b8af",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Splitting into X and Y, and then test and training data."
    },
    {
      "id": "c0bc5326-66f3-4ac1-b809-412406411fb7",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "X and Y data, and Test and Train",
        "title": "X and Y data, and Test and Train"
      },
      "source": "#then split into X and Y \nwq_data_Y = wq_data[[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]]\nwq_data_X = wq_data.drop(columns=[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\", \"Longitude\", \"Latitude\"])\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n    wq_data_X, wq_data_Y, test_size=0.2, random_state=888\n)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8e304fa7-e8fd-48eb-9652-c39c57ba55c5",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Lasso Regression\n\nConducting Lasso Regression on our predictors now. First lets define a pipeline that will scale the data for Lasso, and then fit the model. For each of the three responses, alpha needs to be fine tuned to be the best. In this case, we're using Cross Validation to estimate the best, then fitting the based on that.\n"
    },
    {
      "id": "2cef972c-5aa1-4190-9f2f-81d82e68c5c0",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "\n### Total Alkalinity\nFirst model fitting:"
    },
    {
      "id": "ee9c192d-a982-4616-9348-d04c79271ec0",
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false,
        "language": "python",
        "name": "Lasso - Total Alkalinity",
        "title": "Lasso - Total Alkalinity"
      },
      "source": "# Create a pipeline that first scales the data, then applies Lasso regression\n# The 'alpha' parameter controls the strength of the regularization\nY_train_totalalkalinity = Y_train[[\"Total Alkalinity\"]]\nY_test_totalalkalinity = Y_test[[\"Total Alkalinity\"]]\n\npipeline_totalalkalinity = Pipeline([\n    ('scaler_totalalkalinity', StandardScaler()),\n    ('lasso_totalalkalinity', LassoCV(cv=8)) # You can tune the alpha value\n])\n\n# Fit the model using the training data\npipeline_totalalkalinity.fit(X_train, Y_train_totalalkalinity)\n\n# The model's coefficients can be accessed after fitting\n# The coefficients are for the *scaled* data\nscaled_coefficients_totalalkalinity = pipeline_totalalkalinity.named_steps['lasso_totalalkalinity'].coef_\nintercept_totalalkalinity = pipeline_totalalkalinity.named_steps['lasso_totalalkalinity'].intercept_    # probably also relevant\nalpha_totalalkalinity = pipeline_totalalkalinity.named_steps['lasso_totalalkalinity'].alpha_\n\nprint(\"Scaled Coefficients:\", scaled_coefficients_totalalkalinity)\n\nprint(\"Intercept:\", intercept_totalalkalinity)\n\nprint(\"Alpha:\", alpha_totalalkalinity)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "50103734-c8b8-4c6f-90e4-e843e9c6eca7",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Then evaluation against training and then test set:"
    },
    {
      "id": "0f8e4425-e8bb-4d59-9af3-e5dec9852778",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Evaluation of Model",
        "title": "Evaluation of Model"
      },
      "source": "# Make predictions on the test set\nY_pred_train_totalalkalinity = pipeline_totalalkalinity.predict(X_train)\n\nmse = mean_squared_error(Y_train_totalalkalinity, Y_pred_train_totalalkalinity)\nr2 = r2_score(Y_train_totalalkalinity, Y_pred_train_totalalkalinity)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_totalalkalinity = pipeline_totalalkalinity.predict(X_test)\n\nmse = mean_squared_error(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\nr2 = r2_score(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9af5008e-44a1-403c-be5b-81f88afed769",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Then evaluate residuals"
    },
    {
      "id": "07affa1b-93b8-427c-9ce3-fb6d8e0f80d9",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot residuals",
        "title": "Plot residuals"
      },
      "source": "Y_train_totalalkalinity_plot =  Y_train_totalalkalinity.to_numpy().flatten()\n\nresid_train_total_alkalinity = Y_train_totalalkalinity_plot - Y_pred_train_totalalkalinity\n\nplt.scatter(Y_train_totalalkalinity_plot, resid_train_total_alkalinity)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_total_alkalinity)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "50e47d81-160b-4bf5-bdca-328b92b9a872",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### For Electrical Conductance\n\nFirst model fitting:"
    },
    {
      "id": "c4e9e45b-fc53-432d-85fe-326231a6f6cb",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Lasso - Electrical Conductance",
        "title": "Lasso - Electrical Conductance"
      },
      "source": "# Create a pipeline that first scales the data, then applies Lasso regression\n# The 'alpha' parameter controls the strength of the regularization\nY_train_electricalconductance = Y_train[[\"Electrical Conductance\"]]\nY_test_electricalconductance = Y_test[[\"Electrical Conductance\"]]\n\npipeline_electricalconductance = Pipeline([\n    ('scaler_electricalconductance', StandardScaler()),\n    ('lasso_electricalconductance', LassoCV(cv=8)) # You can tune the alpha value\n])\n\n# Fit the model using the training data\npipeline_electricalconductance.fit(X_train, Y_train_electricalconductance)\n\n# Make predictions on the test set\npredictions_electricalconductance = pipeline_electricalconductance.predict(X_test)\n\n# The model's coefficients can be accessed after fitting\n# The coefficients are for the *scaled* data\nscaled_coefficients_electricalconductance = pipeline_electricalconductance.named_steps['lasso_electricalconductance'].coef_\nintercept_electricalconductance = pipeline_electricalconductance.named_steps['lasso_electricalconductance'].intercept_    # probably also relevant\nalpha_electricalconductance = pipeline_electricalconductance.named_steps['lasso_electricalconductance'].alpha_\n\nprint(\"Scaled Coefficients:\", scaled_coefficients_electricalconductance)\n\nprint(\"Intercept:\", intercept_electricalconductance)\n\nprint(\"Alpha:\", alpha_electricalconductance)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "83a209d1-7f7e-471e-b20e-5712b3ee760e",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Evaluate on the training and test data:\n"
    },
    {
      "id": "eef1fc54-55f6-43d1-8100-fca442c81cf7",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Evaluation of Model",
        "title": "Evaluation of Model"
      },
      "source": "# Make predictions on the test set\nY_pred_train_electricalconductance = pipeline_electricalconductance.predict(X_train)\n\nmse = mean_squared_error(Y_train_electricalconductance, Y_pred_train_electricalconductance)\nr2 = r2_score(Y_train_electricalconductance, Y_pred_train_electricalconductance)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_electricalconductance = pipeline_electricalconductance.predict(X_test)\n\nmse = mean_squared_error(Y_test_electricalconductance, Y_pred_test_electricalconductance)\nr2 = r2_score(Y_test_electricalconductance, Y_pred_test_electricalconductance)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9a946fb3-6c84-4059-aac6-f86e2aec9302",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Then evaluate residuals"
    },
    {
      "id": "5a475759-75eb-4e9e-a1f6-a1828b8adbe9",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot residuals",
        "title": "Plot residuals"
      },
      "source": "Y_train_electricalconductance_plot =  Y_train_electricalconductance.to_numpy().flatten()\n\nresid_train_electricalconductance = Y_train_electricalconductance_plot - Y_pred_train_electricalconductance\n\nplt.scatter(Y_train_electricalconductance_plot, resid_train_electricalconductance)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_electricalconductance)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cccc9608-8663-4406-81b8-da9b1d30b38a",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Dissolved Reactive Phosphorus"
    },
    {
      "id": "fcd93147-12fb-457e-b3a0-5168d276a3db",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Lasso - Dissolved Reactive Phosphorus",
        "title": "Lasso - Dissolved Reactive Phosphorus"
      },
      "source": "# Create a pipeline that first scales the data, then applies Lasso regression\n# The 'alpha' parameter controls the strength of the regularization\nY_train_dissolvedreactivephosphorus = Y_train[[\"Dissolved Reactive Phosphorus\"]]\nY_test_dissolvedreactivephosphorus = Y_test[[\"Dissolved Reactive Phosphorus\"]]\n\npipeline_dissolvedreactivephosphorus = Pipeline([\n    ('scaler_dissolvedreactivephosphorus', StandardScaler()),\n    ('lasso_dissolvedreactivephosphorus', LassoCV(cv=8)) # You can tune the alpha value\n])\n\n# Fit the model using the training data\npipeline_dissolvedreactivephosphorus.fit(X_train, Y_train_dissolvedreactivephosphorus)\n\n# Make predictions on the test set\npredictionspipeline_dissolvedreactivephosphorus = pipeline_dissolvedreactivephosphorus.predict(X_test)\n\n# The model's coefficients can be accessed after fitting\n# The coefficients are for the *scaled* data\nscaled_coefficients_dissolvedreactivephosphorus = pipeline_dissolvedreactivephosphorus.named_steps['lasso_dissolvedreactivephosphorus'].coef_\nintercept_dissolvedreactivephosphorus = pipeline_dissolvedreactivephosphorus.named_steps['lasso_dissolvedreactivephosphorus'].intercept_    # probably also relevant\nalpha_dissolvedreactivephosphorus = pipeline_dissolvedreactivephosphorus.named_steps['lasso_dissolvedreactivephosphorus'].alpha_    # probably also relevant\n\nprint(\"Scaled Coefficients:\", scaled_coefficients_dissolvedreactivephosphorus)\n\nprint(\"Intercept:\", intercept_dissolvedreactivephosphorus)\n\nprint(\"Alpha: \", alpha_dissolvedreactivephosphorus)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "112f9d82-ef91-4387-8246-e141f2b93693",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Evaluate on the training and test data:"
    },
    {
      "id": "6911932b-5f32-4621-9209-58457de17f08",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Evaluation of Model",
        "title": "Evaluation of Model"
      },
      "source": "# Make predictions on the test set\nY_pred_train_dissolvedreactivephosphorus = pipeline_dissolvedreactivephosphorus.predict(X_train)\n\nmse = mean_squared_error(Y_train_dissolvedreactivephosphorus, Y_pred_train_dissolvedreactivephosphorus)\nr2 = r2_score(Y_train_dissolvedreactivephosphorus, Y_pred_train_dissolvedreactivephosphorus)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_dissolvedreactivephosphorus = pipeline_dissolvedreactivephosphorus.predict(X_test)\n\nmse = mean_squared_error(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\nr2 = r2_score(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "be868a2e-71fb-4672-8bb2-923945e1f653",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Now checking residuals"
    },
    {
      "id": "33417dd5-f2a2-4536-8c09-d61c901837f2",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot residuals",
        "title": "Plot residuals"
      },
      "source": "Y_train_dissolvedreactivephosphorus_plot =  Y_train_dissolvedreactivephosphorus.to_numpy().flatten()\n\nresid_train_dissolvedreactivephosphorus = Y_train_dissolvedreactivephosphorus_plot - Y_pred_train_dissolvedreactivephosphorus\n\nplt.scatter(Y_train_dissolvedreactivephosphorus_plot, resid_train_dissolvedreactivephosphorus)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_dissolvedreactivephosphorus)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d8ae0e32-6917-47ca-8edc-fd89494eb53b",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Random Forest Regression\n\nLets build a random forest regression using our data now. We will use RFE and Cross Validation to choose the optimal number of predictors.\n\nRecursive Feature Elimination (RFE) a backward selection, wrapper-based machine learning technique that improves model performance and reduces overfitting by recursively removing the least significant features. In this case, we will iterate through Random Forest Regressions, using RFE to select variables that perform the best, combining with Cross Validation as follows:\n\n1. Iterative RFE inside a Cross-Validation Loop: The data is split into cross-validation folds. In each fold's training set, the RFE process runs multiple times, eliminating a set number or percentage of the least important features at each \"step\".\n2. Performance Evaluation: For each number of features evaluated within the RFE process, the model's performance (e.g., accuracy, F1 score, R-squared) is scored on the corresponding test (validation) fold.\n3. Averaging Scores: The scores for each feature subset size are averaged across all cross-validation folds.\n4. Optimal Feature Selection: The number of features that yields the highest average cross-validation score is identified as the optimal number.\n5. Final RFE Fit: A final RFE process is run on the entire dataset using the determined optimal number of features to select the final feature set\n"
    },
    {
      "id": "f7400bdb-fadd-4207-8366-ecb290ce6407",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Total Alkalinity\n\nFirst lets choose the features, lets choose the features and create the dataframe:"
    },
    {
      "id": "fab5e668-a0db-45bb-a89d-977a6c488c2d",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting all features",
        "title": "Fitting all features"
      },
      "source": "rf_totalalkalinity = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n\nrfecv_totalalkalinity = RFECV(estimator=rf_totalalkalinity\n    , step=1\n    , cv=KFold(8)\n    , scoring='neg_mean_squared_error'\n    , n_jobs=-1)\n    \nrfecv_totalalkalinity.fit(X_train, Y_train_totalalkalinity)\n\nprint(f\"Optimal number of features: {rfecv_totalalkalinity.n_features_}\")\nprint(f\"Selected features mask: {rfecv_totalalkalinity.support_}\")\n\nmean_scores = rfecv_totalalkalinity.cv_results_['mean_test_score']\nnum_features = rfecv_totalalkalinity.cv_results_['n_features']\nstd_error = rfecv_totalalkalinity.cv_results_['std_test_score']\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"MSE\")\nplt.errorbar(\n    x=num_features,\n    y=mean_scores,\n    yerr=std_error,\n)\nplt.title(\"Recursive Feature Elimination Total Alkalinity\")\nplt.show()\n\nselected_features_totalalkalinity = rfecv_totalalkalinity.get_feature_names_out() \nX_train_selected_totalalkalinity = X_train[selected_features_totalalkalinity]\nX_test_selected_totalalkalinity = X_test[selected_features_totalalkalinity]\n\nprint(X_train_selected_totalalkalinity.info())",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c45febb1-f1a2-4049-ae92-4efcde751dac",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Fit a new model on the selected features, and then evaluate its performance:"
    },
    {
      "id": "e72f8e42-ac45-4a62-876b-ffeb8bbd76f9",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting and evaluation a restricted model",
        "title": "Fitting and evaluation a restricted model"
      },
      "source": "rf_totalalkalinity_selected = RandomForestRegressor(n_estimators=100, random_state=42)\n\nrf_totalalkalinity_selected.fit(X_train_selected_totalalkalinity, Y_train_totalalkalinity)\n\n# Make predictions on the test set\nY_pred_train_totalalkalinity = rf_totalalkalinity_selected.predict(X_train_selected_totalalkalinity)\n\nmse = mean_squared_error(Y_train_totalalkalinity, Y_pred_train_totalalkalinity)\nr2 = r2_score(Y_train_totalalkalinity, Y_pred_train_totalalkalinity)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_totalalkalinity = rf_totalalkalinity_selected.predict(X_test_selected_totalalkalinity)\n\nmse = mean_squared_error(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\nr2 = r2_score(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c5a4aa2a-0df6-488b-a0f2-7b36bd8f1f08",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Then check residuals:"
    },
    {
      "id": "39f72f90-50ef-4f73-a2dd-fa9f270ebbcb",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot Residuals",
        "title": "Plot Residuals"
      },
      "source": "Y_train_totalalkalinity_plot =  Y_train_totalalkalinity.to_numpy().flatten()\n\nresid_train_totalalkalinity = Y_train_totalalkalinity_plot - Y_pred_train_totalalkalinity\n\nplt.scatter(Y_train_totalalkalinity_plot, resid_train_totalalkalinity)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_totalalkalinity)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "267b6fc2-973e-480f-9718-b7642515f519",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Electrical Conductance"
    },
    {
      "id": "296500a2-4c1a-4595-bcd0-7efc080139b2",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Ftitting all features",
        "title": "Ftitting all features"
      },
      "source": "rf_electricalconductance = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n\nrfecv_electricalconductance = RFECV(estimator=rf_electricalconductance\n    , step=1\n    , cv=KFold(8)\n    , scoring='neg_mean_squared_error'\n    , n_jobs=-1)\n    \nrfecv_electricalconductance.fit(X_train, Y_train_electricalconductance)\n\nprint(f\"Optimal number of features: {rfecv_electricalconductance.n_features_}\")\nprint(f\"Selected features mask: {rfecv_electricalconductance.support_}\")\n\nmean_scores = rfecv_electricalconductance.cv_results_['mean_test_score']\nnum_features = rfecv_electricalconductance.cv_results_['n_features']\nstd_error = rfecv_electricalconductance.cv_results_['std_test_score']\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"MSE\")\nplt.errorbar(\n    x=num_features,\n    y=mean_scores,\n    yerr=std_error,\n)\nplt.title(\"Recursive Feature Elimination Electrical Conductance\")\nplt.show()\n\nselected_features_electricalconductance = rfecv_electricalconductance.get_feature_names_out() \nX_train_selected_electricalconductance = X_train[selected_features_electricalconductance]\nX_test_selected_electricalconductance = X_test[selected_features_electricalconductance]\n\nprint(X_train_selected_electricalconductance.info())",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d35e357-9be5-447d-b635-a1b661aec9b8",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting and evaluating chosen features",
        "title": "Fitting and evaluating chosen features"
      },
      "source": "rf_electricalconductance_selected = RandomForestRegressor(n_estimators=100, random_state=42)\n\nrf_electricalconductance_selected.fit(X_train_selected_electricalconductance, Y_train_electricalconductance)\n\n# Make predictions on the test set\nY_pred_train_electricalconductance = rf_electricalconductance_selected.predict(X_train_selected_electricalconductance)\n\nmse = mean_squared_error(Y_train_electricalconductance, Y_pred_train_electricalconductance)\nr2 = r2_score(Y_train_electricalconductance, Y_pred_train_electricalconductance)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_electricalconductance = rf_electricalconductance_selected.predict(X_test_selected_electricalconductance)\n\nmse = mean_squared_error(Y_test_electricalconductance, Y_pred_test_electricalconductance)\nr2 = r2_score(Y_test_electricalconductance, Y_pred_test_electricalconductance)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e58d572b-2877-420b-8f79-ec7e7fc23995",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot Residuals",
        "title": "Plot Residuals"
      },
      "source": "Y_train_electricalconductance_plot =  Y_train_electricalconductance.to_numpy().flatten()\n\nresid_train_electricalconductance = Y_train_electricalconductance_plot - Y_pred_train_electricalconductance\n\nplt.scatter(Y_train_electricalconductance_plot, resid_train_electricalconductance)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_electricalconductance)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "80e1b4d5-590f-48a7-8724-f381688eadff",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Dissolved Reactive Phosphorus\n\n"
    },
    {
      "id": "745022e2-2646-4979-a7eb-49b923f89bcc",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Ftitting all features",
        "title": "Ftitting all features"
      },
      "source": "rf_dissolvedreactivephosphorus = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n\nrfecv_dissolvedreactivephosphorus = RFECV(estimator=rf_dissolvedreactivephosphorus\n    , step=1\n    , cv=KFold(8)\n    , scoring='neg_mean_squared_error'\n    , n_jobs=-1)\n    \nrfecv_dissolvedreactivephosphorus.fit(X_train, Y_train_dissolvedreactivephosphorus)\n\nprint(f\"Optimal number of features: {rfecv_dissolvedreactivephosphorus.n_features_}\")\nprint(f\"Selected features mask: {rfecv_dissolvedreactivephosphorus.support_}\")\n\nmean_scores = rfecv_dissolvedreactivephosphorus.cv_results_['mean_test_score']\nnum_features = rfecv_dissolvedreactivephosphorus.cv_results_['n_features']\nstd_error = rfecv_dissolvedreactivephosphorus.cv_results_['std_test_score']\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"MSE\")\nplt.errorbar(\n    x=num_features,\n    y=mean_scores,\n    yerr=std_error,\n)\nplt.title(\"Recursive Feature Elimination Dissolved Reactive Phosphophrus\")\nplt.show()\n\nselected_features_dissolvedreactivephosphorus = rfecv_dissolvedreactivephosphorus.get_feature_names_out() \nX_train_selected_dissolvedreactivephosphorus = X_train[selected_features_dissolvedreactivephosphorus]\nX_test_selected_dissolvedreactivephosphorus = X_test[selected_features_dissolvedreactivephosphorus]\n\nprint(X_train_selected_dissolvedreactivephosphorus.info())",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0b9bbd0-d976-4164-9370-b7fd8d3fbd37",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting and Evaluating Selected Feautres",
        "title": "Fitting and Evaluating Selected Feautres"
      },
      "source": "rf_dissolvedreactivephosphorus_selected = RandomForestRegressor(n_estimators=100, random_state=42)\n\nrf_dissolvedreactivephosphorus_selected.fit(X_train_selected_dissolvedreactivephosphorus, Y_train_dissolvedreactivephosphorus)\n\n# Make predictions on the test set\nY_pred_train_dissolvedreactivephosphorus = rf_dissolvedreactivephosphorus_selected.predict(X_train_selected_dissolvedreactivephosphorus)\n\nmse = mean_squared_error(Y_train_dissolvedreactivephosphorus, Y_pred_train_dissolvedreactivephosphorus)\nr2 = r2_score(Y_train_dissolvedreactivephosphorus, Y_pred_train_dissolvedreactivephosphorus)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_dissolvedreactivephosphorus = rf_dissolvedreactivephosphorus_selected.predict(X_test_selected_dissolvedreactivephosphorus)\n\nmse = mean_squared_error(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\nr2 = r2_score(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "37b26624-a2d7-4647-a77b-47cf19011cd8",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plotting residuals",
        "title": "Plotting residuals"
      },
      "source": "Y_train_dissolvedreactivephosphorus_plot =  Y_train_dissolvedreactivephosphorus.to_numpy().flatten()\n\nresid_train_dissolvedreactivephosphorus = Y_train_dissolvedreactivephosphorus_plot - Y_pred_train_dissolvedreactivephosphorus\n\nplt.scatter(Y_train_dissolvedreactivephosphorus_plot, resid_train_dissolvedreactivephosphorus)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_dissolvedreactivephosphorus)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c5fb5425-2e93-4e11-b014-204d26ec2fbf",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## ScratchPad\n\nNot sure I believe the R^2, lets check the fit?\n\nALSO: LONGITUDE AND LATITIDE ARE THE STRONGEST PREDICTORS IN BOTH LASSO AND A RANDOM FORREST? HUH"
    },
    {
      "id": "f858a1ac-31e3-4e61-b9b3-4827f78bbc19",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "TA",
        "title": "TA"
      },
      "source": "plt.scatter(Y_train_totalalkalinity_plot, Y_pred_train_totalalkalinity)\nplt.xlabel('Total Alkalinity Train')\nplt.ylabel('Predict')\nplt.show()\n\nplt.scatter(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\nplt.xlabel('Test')\nplt.ylabel('Total Alkalinity Predict')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97f85c35-c3ac-4905-899a-f65d24f7c315",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "EC",
        "title": "EC"
      },
      "source": "plt.scatter(Y_train_electricalconductance_plot, Y_pred_train_electricalconductance)\nplt.xlabel('Electrical Conductance Train')\nplt.ylabel('Predict')\nplt.show()\n\nplt.scatter(Y_test_electricalconductance, Y_pred_test_electricalconductance)\nplt.xlabel('Electrical Conductance Test')\nplt.ylabel('Predict')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "372d3b45-37f1-4f46-92ab-d38ed0c8fc83",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "DRP",
        "title": "DRP"
      },
      "source": "plt.scatter(Y_train_dissolvedreactivephosphorus_plot, Y_pred_train_dissolvedreactivephosphorus)\nplt.xlabel('Dissolved Reactive Phosphorus Train')\nplt.ylabel('Predict')\nplt.show()\n\nplt.scatter(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\nplt.xlabel('Dissolved Reactive Phosphorus Test')\nplt.ylabel('Predict')\nplt.show()",
      "outputs": [],
      "execution_count": null
    }
  ]
}
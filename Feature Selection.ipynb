{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f28345fb-d896-453d-a825-2bdd518ba2a4",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "# Feature Selection \n\nThis notebook functions as a feature selector for each of the three response variables. To use this: \n\n1. Feed in a dataframe with all of the features that would like to be tested\n2. Run the code in this Notebook\n\nThis should return the features for each of the three response variables that are the best predictors.",
      "execution_count": null
    },
    {
      "id": "9533a38b-77ab-4741-ba18-aec16e64aca5",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Notes and Literature:\n\nLasso is generally better than most other methods for feature fitting. It will return a list of features, and the remainder it 'shrinks' to 0, as such can avoids overfitting - and tests better on out of sample data (prediction). The downside is that it assumes a linear relationship between the variables (variables can be transformed, but after transformation it produces a linear fit). It can be fine tuned with\n 1. Choice of Alpha (higher alpha gives more penalization, shrinking more features to 0)\n 2. Transformation of response variable (GLMs)\n\nRandom Forest is a non-linear approach to the data, but will not give a set of features. It does list the features based on importance, cut offs are based on other rules that need to be applied by the user. Tuning is given by:\n 1. n_estimators, which is the number of estimators in the tree (Although this is moreso a tradeoff on performance not model tuning)\n 2. Where to cut features off\n\nDifferent choices can give different results, so for each set of features, pick the tuning that gives the model w.r.t the chosen criteria"
    },
    {
      "id": "7efc3b8c-7761-4079-82cc-c2ac1b74a9f0",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Preparation\n\nJust running through our feature selection now\n\n### Loading packages / libraries"
    },
    {
      "id": "f1cbbd84-acac-4721-8a21-bf04593f6f4d",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Installing requirements",
        "title": "Installing requirements"
      },
      "source": "!pip install uv\n!uv pip install  -r requirements.txt \n\n#new library\n!pip install mlxtend\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7c24ed8b-7a47-4d65-bc0f-a0df75b99b8f",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Restart Kernel here\n\nThen we load in packages"
    },
    {
      "id": "fa0d7c03-169d-47c9-ba10-97365a11f47c",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Install Packages",
        "title": "Install Packages"
      },
      "source": "## import packages\n\nimport snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\n# Geospatial raster data handling with CRS support\nimport rioxarray as rxr\n\n# Raster operations and spatial windowing\nimport rasterio\nfrom rasterio.windows import Window\n\n# Feature preprocessing and data splitting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom scipy.spatial import cKDTree\n\n# Machine Learning\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\nfrom odc.stac import stac_load\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os \n\n#NEW PACKAGES\nimport planetary_computer \nimport dask \nfrom scipy import stats\nfrom datetime import datetime\nfrom dask.distributed import Client\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LassoCV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\n\nimport xgboost as xgb\n\nfrom sklearn.model_selection import GroupKFold\n\ndef run_groupkfold_cv(X, y, groups, n_splits=5, param_name=\"Parameter\"):\n    gkf = GroupKFold(n_splits=n_splits)\n    fold_results = []\n\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n        # print(f\"\\n=== Fold {fold+1} ===\")\n\n        # Split\n        X_train, X_test = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Scale\n        X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test)\n\n        # Train\n        model = train_model(X_train_scaled, y_train)\n\n        # Evaluate (in-sample)\n        y_train_pred, r2_train, rmse_t\n        # Evaluate (out-sample)rain = evaluate_model(model, X_train_scaled, y_train, \"Train\")\n\n        y_test_pred, r2_test, rmse_test = evaluate_model(model, X_test_scaled, y_test, \"Test\")\n\n        fold_results.append((r2_train, rmse_train, r2_test, rmse_test))\n\n    df_results_kfold = pd.DataFrame(fold_results, columns=['R2_Train', 'RMSE_Train', 'R2_Test', 'RMSE_Test']).reset_index().rename(columns={\"index\": \"fold\"})\n    df_results_kfold['Parameter'] = param_name\n    df_results_kfold['Features'] = ', '.join([col for col in X.columns if col != 'sample_location_group'])\n    df_results_kfold = df_results_kfold[['Parameter', 'Features', 'R2_Train', 'RMSE_Train', 'R2_Test', 'RMSE_Test']]\n\n    return df_results_kfold\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9378bd87-7c7d-462c-a68d-88fdea3ac732",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Loading in Data\n\nLoading in dataframe for model selection\n\nN.B when applying custom dataset, load it in as wq_data below. This has some transformations to scrap junk data and remove all nulls. This may or may not be requried in other functions"
    },
    {
      "id": "9e6e9679-71ba-4e01-9eb1-e94ad43e8592",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Loading in all data",
        "title": "Loading in all data"
      },
      "source": "Water_Quality_df = pd.read_csv(\"data/water_quality_training_dataset.csv\")\n\nlandsat_train_features = pd.read_csv(\"data/landsat/landsat_features_training_mvdb.csv\")\nlandsat_train_features['Sample Date'] = pd.to_datetime(landsat_train_features['Sample Date'], format=\"%d/%m/%Y\")\n\nTerraclimate_df = pd.read_csv(\"data/terraclimate/terraclimate_features_training_pet.csv\")\n\nq_terraclimateload = pd.read_csv(\"data/terraclimate/terraclimate_features_training_q.csv\")\nq_terraclimate = q_terraclimateload[['LATITUDE', 'LONGITUDE', 'SAMPLE DATE', 'Q']].rename(columns={\n    'LATITUDE': 'Latitude',\n    'LONGITUDE': 'Longitude',\n    'SAMPLE DATE': 'Sample Date',\n    '2.Q': 'Q'\n})\n\n\n#landsat_train_features['NDMI'] = landsat_train_features['NDMI'].astype(float)\n#landsat_train_features['MNDWI'] = landsat_train_features['MNDWI'].astype(float)\n#landsat_train_features['Sample Date'] = pd.to_datetime(landsat_train_features['Sample Date'],  format='%d-%m-%Y')\n\ndef combine_two_datasets(dataset1,dataset2,dataset3, dataset4):\n    '''\n    Returns a  vertically concatenated dataset.\n    Attributes:\n    dataset1 - Dataset 1 to be combined \n    dataset2 - Dataset 2 to be combined\n    '''\n    \n    data = pd.concat([dataset1,dataset2,dataset3, dataset4], axis=1)\n    data = data.loc[:, ~data.columns.duplicated()]\n    return data\n\nwq_data = combine_two_datasets(Water_Quality_df, landsat_train_features, Terraclimate_df, q_terraclimate)\nwq_data['Sample Date'] = pd.to_datetime(wq_data['Sample Date'],  format='mixed')\n\n#ullify all negative observations\nfor column in wq_data.columns:\n    if wq_data[column].dtype == 'string': wq_data[column] = pd.to_numeric(wq_data[column], errors='coerce')\n    elif column != \"Sample Date\": wq_data[wq_data[column] < -0.1][column] = np.nan \n    \n#number of cv groups\ncv_groups = 6\n\nwq_data = wq_data.drop(columns=['qa_radsat', 'cloud_qa', 'Unnamed: 0'])\nwq_data = wq_data.dropna(how='any',axis=0)\n\n#split over longitude\nwq_data['cv_group'] = pd.qcut(wq_data['Sample Date'], q=cv_groups, labels=False)\n\nplt.scatter(wq_data['Sample Date'], wq_data['cv_group'])\nplt.xlabel('Sample Date')\nplt.ylabel('cv_group')\nplt.show()\n\nwq_data['Month_cosine'] = np.cos((wq_data['Sample Date'].dt.month + (wq_data['Sample Date'].dt.day/31))* np.pi / 6)\nplt.scatter(wq_data['Sample Date'], wq_data['Month_cosine'])\nplt.xlabel('Sample Date')\nplt.ylabel('Month_Cosine')\nplt.show()\n\nwq_data = wq_data.drop(columns=['Sample Date'])\n\nprint(wq_data.info())\n\n# Specify the number of folds\nlat_sep_kf = GroupKFold(n_splits=cv_groups - 1)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "554a1076-6c6c-490f-a0ef-31a4f1cc09eb",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "This next section is to apply any transformations to the dataset. For example we take the box-cox transformation of the Y data. And for the predictor lwir.1 we take the square. \n\nBut this transformation is arbitrary. Fit any transformations here\n\n"
    },
    {
      "id": "9d8791b5-3ae9-481f-b80e-4419597b3e2f",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Transformation of Variables",
        "title": "Transformation of Variables"
      },
      "source": "#Box Cox of predictors\nTotal_Alkalinity_bc, Total_Alkalinity_lambda_opt = stats.boxcox(wq_data['Total Alkalinity'])\nElectrical_Conductance_bc, Electrical_Conductance_lambda_opt = stats.boxcox(wq_data['Electrical Conductance'])\nDissolved_Reactive_Phosphorus_bc, Dissolved_Reactive_Phosphorus_lambda_opt = stats.boxcox(wq_data['Dissolved Reactive Phosphorus'])\n\nwq_data['Total Alkalinity'] = Total_Alkalinity_bc\nwq_data['Electrical Conductance'] = Electrical_Conductance_bc\nwq_data['Dissolved Reactive Phosphorus'] = Dissolved_Reactive_Phosphorus_bc\n\n#Square x value\nsquarelwir1 = wq_data['lwir.1'] ** 2\nwq_data['lwir.1'] = squarelwir1",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8aa213cb-d4f3-4cb8-bd92-b52447b2b8af",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Splitting into X and Y, and then test and training data."
    },
    {
      "id": "c0bc5326-66f3-4ac1-b809-412406411fb7",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "X and Y data, and Test and Train",
        "title": "X and Y data, and Test and Train"
      },
      "source": "\n#test train based on location\nwq_data_test = wq_data[wq_data['cv_group'] == 0]\nwq_data_train = wq_data[wq_data['cv_group'] > 0]\n\n#then split into X and Y \nY_train = wq_data_train[[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]]\nX_train = wq_data_train.drop(columns=[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\", \"Longitude\", \"Latitude\"])\nY_test = wq_data_test[[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]]\nX_test = wq_data_test.drop(columns=[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\", \"Longitude\", \"Latitude\"])\n\n'''\nY_data = wq_data[[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]]\nX_data = wq_data.drop(columns=[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\", \"Longitude\", \"Latitude\"])\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X_data, Y_data, test_size=0.2, random_state=88, shuffle=False\n)\n'''\n\n\nprint(Y_train.shape)\nprint(X_train.shape)\nprint(Y_test.shape)\nprint(X_test.shape)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d8ae0e32-6917-47ca-8edc-fd89494eb53b",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Random Forest Regression\n\nLets build a random forest regression using our data now. We will use RFE and Cross Validation to choose the optimal number of predictors.\n\nRecursive Feature Elimination (RFE) a backward selection, wrapper-based machine learning technique that improves model performance and reduces overfitting by recursively removing the least significant features. In this case, we will iterate through Random Forest Regressions, using RFE to select variables that perform the best, combining with Cross Validation as follows:\n\n1. Iterative RFE inside a Cross-Validation Loop: The data is split into cross-validation folds. In each fold's training set, the RFE process runs multiple times, eliminating a set number or percentage of the least important features at each \"step\".\n2. Performance Evaluation: For each number of features evaluated within the RFE process, the model's performance (e.g., accuracy, F1 score, R-squared) is scored on the corresponding test (validation) fold.\n3. Averaging Scores: The scores for each feature subset size are averaged across all cross-validation folds.\n4. Optimal Feature Selection: The number of features that yields the highest average cross-validation score is identified as the optimal number.\n5. Final RFE Fit: A final RFE process is run on the entire dataset using the determined optimal number of features to select the final feature set\n"
    },
    {
      "id": "f7400bdb-fadd-4207-8366-ecb290ce6407",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Total Alkalinity\n\nFirst lets choose the features, lets choose the features and create the dataframe:"
    },
    {
      "id": "fab5e668-a0db-45bb-a89d-977a6c488c2d",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting all features",
        "title": "Fitting all features"
      },
      "source": "Y_train_totalalkalinity = Y_train[[\"Total Alkalinity\"]]\nY_test_totalalkalinity = Y_test[[\"Total Alkalinity\"]]\n\n\nrf_totalalkalinity =  xgb.XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=5,\n    subsample=0.3,\n    objective='reg:squarederror',\n    random_state=88,\n    alpha = 1,\n    reg_lambda = 1\n)\n'''\nrf_totalalkalinity =  RandomForestRegressor(\n    n_estimators=100,\n    random_state=88,\n    max_features=0.5\n)\n'''\nrfecv_totalalkalinity = RFECV(estimator=rf_totalalkalinity\n    , step=1\n    , cv=lat_sep_kf\n    , scoring='neg_mean_squared_error'\n    , n_jobs=-1)\n'''\nrfecv_totalalkalinity = RFE(estimator=rf_totalalkalinity\n    , step=1\n)\n'''\n#rfecv_totalalkalinity.fit(X_train.drop(columns=['cv_group']), Y_train_totalalkalinity)    \nrfecv_totalalkalinity.fit(X_train.drop(columns=['cv_group']), Y_train_totalalkalinity, groups= X_train['cv_group'])\n\nprint(f\"Optimal number of features: {rfecv_totalalkalinity.n_features_}\")\nprint(f\"Selected features mask: {rfecv_totalalkalinity.support_}\")\n\n\nmean_scores = rfecv_totalalkalinity.cv_results_['mean_test_score']\nnum_features = rfecv_totalalkalinity.cv_results_['n_features']\nstd_error = rfecv_totalalkalinity.cv_results_['std_test_score']\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"MSE\")\nplt.errorbar(\n    x=num_features,\n    y=mean_scores,\n    yerr=std_error,\n)\nplt.title(\"Recursive Feature Elimination Total Alkalinity\")\nplt.show()\n\nselected_features_totalalkalinity = rfecv_totalalkalinity.get_feature_names_out() \nX_train_selected_totalalkalinity = X_train[selected_features_totalalkalinity]\nX_test_selected_totalalkalinity = X_test[selected_features_totalalkalinity]\n\nprint(X_train_selected_totalalkalinity.info())",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c45febb1-f1a2-4049-ae92-4efcde751dac",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Fit a new model on the selected features, and then evaluate its performance:"
    },
    {
      "id": "e72f8e42-ac45-4a62-876b-ffeb8bbd76f9",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting and evaluation a restricted model",
        "title": "Fitting and evaluation a restricted model"
      },
      "source": "rf_totalalkalinity_selected = xgb.XGBRegressor(\n    n_estimators=300,\n    learning_rate=0.01,\n    max_depth=8,\n    subsample=0.5,\n    objective='reg:squarederror',\n    random_state=88,\n    alpha = 1,\n    reg_lambda = 1\n)\n'''\nrf_totalalkalinity_selected = RandomForestRegressor(n_estimators = 100, random_state = 88, max_features = 0.5)\n'''\nrf_totalalkalinity_selected.fit(X_train_selected_totalalkalinity, Y_train_totalalkalinity)\n\n# Make predictions on the test set\nY_pred_train_totalalkalinity = rf_totalalkalinity_selected.predict(X_train_selected_totalalkalinity)\n\nmse = mean_squared_error(Y_train_totalalkalinity, Y_pred_train_totalalkalinity)\nr2 = r2_score(Y_train_totalalkalinity, Y_pred_train_totalalkalinity)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_totalalkalinity = rf_totalalkalinity_selected.predict(X_test_selected_totalalkalinity)\n\nmse = mean_squared_error(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\nr2 = r2_score(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c5a4aa2a-0df6-488b-a0f2-7b36bd8f1f08",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Then check residuals:"
    },
    {
      "id": "39f72f90-50ef-4f73-a2dd-fa9f270ebbcb",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot Residuals",
        "title": "Plot Residuals"
      },
      "source": "Y_train_totalalkalinity_plot =  Y_train_totalalkalinity.to_numpy().flatten()\n\nresid_train_totalalkalinity = Y_train_totalalkalinity_plot - Y_pred_train_totalalkalinity\n\nplt.scatter(Y_train_totalalkalinity_plot, resid_train_totalalkalinity)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_totalalkalinity)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f858a1ac-31e3-4e61-b9b3-4827f78bbc19",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "TA",
        "title": "TA"
      },
      "source": "slope, intercept = np.polyfit(Y_train_totalalkalinity_plot, Y_pred_train_totalalkalinity, 1) #\nline_of_best_fit = slope * Y_train_totalalkalinity_plot + intercept\nplt.scatter(Y_train_totalalkalinity_plot, Y_pred_train_totalalkalinity)\nplt.plot(Y_train_totalalkalinity_plot, line_of_best_fit, color='red', label='Line of Best Fit') #\nplt.xlabel('Total Alkalinity Train')\nplt.ylabel('Predict')\nplt.show()\n\nslope, intercept = np.polyfit(Y_test_totalalkalinity.to_numpy().flatten(), Y_pred_test_totalalkalinity, 1) #\nline_of_best_fit = slope * Y_test_totalalkalinity.to_numpy().flatten() + intercept\nplt.scatter(Y_test_totalalkalinity, Y_pred_test_totalalkalinity)\nplt.plot(Y_test_totalalkalinity, line_of_best_fit, color='red', label='Line of Best Fit') #\nplt.xlabel('Total Alkalinity Test')\nplt.ylabel('Predict')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "267b6fc2-973e-480f-9718-b7642515f519",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Electrical Conductance"
    },
    {
      "id": "296500a2-4c1a-4595-bcd0-7efc080139b2",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Ftitting all features",
        "title": "Ftitting all features"
      },
      "source": "Y_train_electricalconductance = Y_train[[\"Electrical Conductance\"]]\nY_test_electricalconductance = Y_test[[\"Electrical Conductance\"]]\n\n\nrf_electricalconductance =  xgb.XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=5,\n    subsample=0.3,\n    objective='reg:squarederror',\n    random_state=88,\n    alpha = 1,\n    reg_lambda = 1\n)\n'''\nrf_electricalconductance =  RandomForestRegressor(\n    n_estimators=100,\n    random_state=88,\n    max_features=0.5\n)\n'''\nrfecv_electricalconductance = RFECV(estimator=rf_electricalconductance\n    , step=1\n    , cv=lat_sep_kf\n    , scoring='neg_mean_squared_error'\n    , n_jobs=-1)\n    \nrfecv_electricalconductance.fit(X_train.drop(columns=['cv_group']), Y_train_electricalconductance, groups= X_train['cv_group'])\n\nprint(f\"Optimal number of features: {rfecv_electricalconductance.n_features_}\")\nprint(f\"Selected features mask: {rfecv_electricalconductance.support_}\")\n\nmean_scores = rfecv_electricalconductance.cv_results_['mean_test_score']\nnum_features = rfecv_electricalconductance.cv_results_['n_features']\nstd_error = rfecv_electricalconductance.cv_results_['std_test_score']\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"MSE\")\nplt.errorbar(\n    x=num_features,\n    y=mean_scores,\n    yerr=std_error,\n)\nplt.title(\"Recursive Feature Elimination Electrical Conductance\")\nplt.show()\n\nselected_features_electricalconductance = rfecv_electricalconductance.get_feature_names_out() \nX_train_selected_electricalconductance = X_train[selected_features_electricalconductance]\nX_test_selected_electricalconductance = X_test[selected_features_electricalconductance]\n\nprint(X_train_selected_electricalconductance.info())",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d35e357-9be5-447d-b635-a1b661aec9b8",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting and evaluating chosen features",
        "title": "Fitting and evaluating chosen features"
      },
      "source": "\nrf_electricalconductance_selected = xgb.XGBRegressor(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.5,\n    objective='reg:squarederror',\n    random_state=88,\n    alpha = 1,\n    reg_lambda = 1\n)\n'''\nrf_electricalconductance_selected = RandomForestRegressor(n_estimators = 100, random_state = 88, max_features = 0.5)\n'''\nrf_electricalconductance_selected.fit(X_train_selected_electricalconductance, Y_train_electricalconductance)\n\n# Make predictions on the test set\nY_pred_train_electricalconductance = rf_electricalconductance_selected.predict(X_train_selected_electricalconductance)\n\nmse = mean_squared_error(Y_train_electricalconductance, Y_pred_train_electricalconductance)\nr2 = r2_score(Y_train_electricalconductance, Y_pred_train_electricalconductance)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_electricalconductance = rf_electricalconductance_selected.predict(X_test_selected_electricalconductance)\n\nmse = mean_squared_error(Y_test_electricalconductance, Y_pred_test_electricalconductance)\nr2 = r2_score(Y_test_electricalconductance, Y_pred_test_electricalconductance)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e58d572b-2877-420b-8f79-ec7e7fc23995",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plot Residuals",
        "title": "Plot Residuals"
      },
      "source": "Y_train_electricalconductance_plot =  Y_train_electricalconductance.to_numpy().flatten()\n\nresid_train_electricalconductance = Y_train_electricalconductance_plot - Y_pred_train_electricalconductance\n\nplt.scatter(Y_train_electricalconductance_plot, resid_train_electricalconductance)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_electricalconductance)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97f85c35-c3ac-4905-899a-f65d24f7c315",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "EC",
        "title": "EC"
      },
      "source": "slope, intercept = np.polyfit(Y_train_electricalconductance_plot, Y_pred_train_electricalconductance, 1) #\nline_of_best_fit = slope * Y_train_electricalconductance_plot + intercept\nplt.scatter(Y_train_electricalconductance_plot, Y_pred_train_electricalconductance)\nplt.plot(Y_train_electricalconductance_plot, line_of_best_fit, color='red', label='Line of Best Fit') #\nplt.xlabel('Electrical Conductance Train')\nplt.ylabel('Predict')\nplt.show()\n\nslope, intercept = np.polyfit(Y_test_electricalconductance.to_numpy().flatten(), Y_pred_test_electricalconductance, 1) #\nline_of_best_fit = slope * Y_test_electricalconductance.to_numpy().flatten() + intercept\nplt.scatter(Y_test_electricalconductance, Y_pred_test_electricalconductance)\nplt.plot(Y_test_electricalconductance, line_of_best_fit, color='red', label='Line of Best Fit') #\nplt.xlabel('Electrical Conductance Test')\nplt.ylabel('Predict')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "80e1b4d5-590f-48a7-8724-f381688eadff",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Dissolved Reactive Phosphorus\n\n"
    },
    {
      "id": "8ec70ac7-4bda-4af9-bf25-d08d506d64c6",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "745022e2-2646-4979-a7eb-49b923f89bcc",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Ftitting all features",
        "title": "Ftitting all features"
      },
      "source": "Y_train_dissolvedreactivephosphorus = Y_train[[\"Dissolved Reactive Phosphorus\"]]\nY_test_dissolvedreactivephosphorus = Y_test[[\"Dissolved Reactive Phosphorus\"]]\n\n\nrf_dissolvedreactivephosphorus = xgb.XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=5,\n    subsample=0.3,\n    objective='reg:squarederror',\n    random_state=88,\n    alpha = 1,\n    reg_lambda = 1\n)\n'''\nrf_dissolvedreactivephosphorus = RandomForestRegressor(\n    n_estimators = 100\n    , random_state = 88\n    , max_features = 0.5\n)    \n'''\nrfecv_dissolvedreactivephosphorus = RFECV(estimator=rf_dissolvedreactivephosphorus\n    , step=1\n    , cv=lat_sep_kf\n    , scoring='neg_mean_squared_error'\n    , n_jobs=-1)\n    \nrfecv_dissolvedreactivephosphorus.fit(X_train.drop(columns=['cv_group']), Y_train_dissolvedreactivephosphorus, groups= X_train['cv_group'])\n\nprint(f\"Optimal number of features: {rfecv_dissolvedreactivephosphorus.n_features_}\")\nprint(f\"Selected features mask: {rfecv_dissolvedreactivephosphorus.support_}\")\n\nmean_scores = rfecv_dissolvedreactivephosphorus.cv_results_['mean_test_score']\nnum_features = rfecv_dissolvedreactivephosphorus.cv_results_['n_features']\nstd_error = rfecv_dissolvedreactivephosphorus.cv_results_['std_test_score']\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"MSE\")\nplt.errorbar(\n    x=num_features,\n    y=mean_scores,\n    yerr=std_error,\n)\nplt.title(\"Recursive Feature Elimination Dissolved Reactive Phosphophrus\")\nplt.show()\n\nselected_features_dissolvedreactivephosphorus = rfecv_dissolvedreactivephosphorus.get_feature_names_out() \nX_train_selected_dissolvedreactivephosphorus = X_train[selected_features_dissolvedreactivephosphorus]\nX_test_selected_dissolvedreactivephosphorus = X_test[selected_features_dissolvedreactivephosphorus]\n\nprint(X_train_selected_dissolvedreactivephosphorus.info())",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0b9bbd0-d976-4164-9370-b7fd8d3fbd37",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Fitting and Evaluating Selected Feautres",
        "title": "Fitting and Evaluating Selected Feautres"
      },
      "source": "\nrf_dissolvedreactivephosphorus_selected = xgb.XGBRegressor(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.5,\n    objective='reg:squarederror',\n    random_state=88,\n    alpha = 1,\n    reg_lambda = 1\n)\n'''\nrf_dissolvedreactivephosphorus_selected = RandomForestRegressor(n_estimators = 100, random_state = 88, max_features = 0.5)\n'''\nrf_dissolvedreactivephosphorus_selected.fit(X_train_selected_dissolvedreactivephosphorus, Y_train_dissolvedreactivephosphorus)\n\n# Make predictions on the test set\nY_pred_train_dissolvedreactivephosphorus = rf_dissolvedreactivephosphorus_selected.predict(X_train_selected_dissolvedreactivephosphorus)\n\nmse = mean_squared_error(Y_train_dissolvedreactivephosphorus, Y_pred_train_dissolvedreactivephosphorus)\nr2 = r2_score(Y_train_dissolvedreactivephosphorus, Y_pred_train_dissolvedreactivephosphorus)\n\nprint(\"Training: MSE:\", mse, \"R2:\", r2)\n\n# Make predictions on the test set\nY_pred_test_dissolvedreactivephosphorus = rf_dissolvedreactivephosphorus_selected.predict(X_test_selected_dissolvedreactivephosphorus)\n\nmse = mean_squared_error(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\nr2 = r2_score(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\n\nprint(\"Test: MSE:\", mse, \"R2:\", r2)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "37b26624-a2d7-4647-a77b-47cf19011cd8",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Plotting residuals",
        "title": "Plotting residuals"
      },
      "source": "Y_train_dissolvedreactivephosphorus_plot =  Y_train_dissolvedreactivephosphorus.to_numpy().flatten()\n\nresid_train_dissolvedreactivephosphorus = Y_train_dissolvedreactivephosphorus_plot - Y_pred_train_dissolvedreactivephosphorus\n\nplt.scatter(Y_train_dissolvedreactivephosphorus_plot, resid_train_dissolvedreactivephosphorus)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Total Alkalinity')\nplt.ylabel('Residuals')\nplt.show()\n\nsm.qqplot(resid_train_dissolvedreactivephosphorus)\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "372d3b45-37f1-4f46-92ab-d38ed0c8fc83",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "DRP",
        "title": "DRP"
      },
      "source": "\nslope, intercept = np.polyfit(Y_train_dissolvedreactivephosphorus_plot, Y_pred_train_dissolvedreactivephosphorus, 1) #\nline_of_best_fit = slope * Y_train_dissolvedreactivephosphorus_plot + intercept\n\nplt.scatter(Y_train_dissolvedreactivephosphorus_plot, Y_pred_train_dissolvedreactivephosphorus)\nplt.plot(Y_train_dissolvedreactivephosphorus_plot, line_of_best_fit, color='red', label='Line of Best Fit') #\nplt.xlabel('Dissolved Reactive Phosphorus Train')\nplt.ylabel('Predict')\nplt.show()\n\n\nslope, intercept = np.polyfit(Y_test_dissolvedreactivephosphorus.to_numpy().flatten(), Y_pred_test_dissolvedreactivephosphorus, 1) #\nline_of_best_fit = slope * Y_test_dissolvedreactivephosphorus.to_numpy().flatten() + intercept\nplt.scatter(Y_test_dissolvedreactivephosphorus, Y_pred_test_dissolvedreactivephosphorus)\nplt.plot(Y_test_dissolvedreactivephosphorus, line_of_best_fit, color='red', label='Line of Best Fit') #\nplt.xlabel('Dissolved Reactive Phosphorus Test')\nplt.ylabel('Predict')\nplt.show()",
      "outputs": [],
      "execution_count": null
    }
  ]
}
{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "bc4c373d-cac7-4636-bc38-d1f34494d5fb",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Exploratory Data Analysis\n\nThis notebook contains EDA on the initial dataset to determine any initial correlations, determine any erroneous data points and propose an initial model fitting subject to external analysis and variables.\n\n## THIS NOTEBOOK IS INCOMPLETE. FURTHER STEPS:\n 1. Consider any transformations of the landsat data that might aid in increasing correlation\n 2. See if terraclimate data can be extracted and mapped against the validation and test set\n 3. And if 2 is possible, repeat multivariate analysis for that\n"
    },
    {
      "id": "6e36c308-a178-4897-a894-bc110f44385c",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Preparation\n\nBefore we start our EDA, just re-importing our packages and re-defining our data frames.\n\n### Requirements and Packages"
    },
    {
      "cell_type": "code",
      "id": "9327ae62-a9c1-4878-b670-36f2b15b529a",
      "metadata": {
        "language": "python",
        "name": "install requirements",
        "title": "install requirements"
      },
      "source": "!pip install uv\n!uv pip install  -r requirements.txt ",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "877c08c1-d138-49e6-a792-8e71ffe7294e",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "import packages",
        "title": "import packages"
      },
      "source": "## import packages\n\nimport snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\n# Geospatial raster data handling with CRS support\nimport rioxarray as rxr\n\n# Raster operations and spatial windowing\nimport rasterio\nfrom rasterio.windows import Window\n\n# Feature preprocessing and data splitting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom scipy.spatial import cKDTree\n\n# Machine Learning\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\nfrom odc.stac import stac_load\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os \n\n\n#NEW PACKAGES\nimport planetary_computer \nimport dask \nfrom scipy import stats\nfrom datetime import datetime\nfrom dask.distributed import Client",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "72ed40d2-389e-4223-a9de-bb728da00cfb",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Loading in data\n\nWe need to get this to work - terradata to map to lat/long?"
    },
    {
      "id": "00e43f31-db72-486d-ab54-e6dc873d6dcd",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "This code is all avaliable in benchmark_model_notebook_snowflake.ipynt"
    },
    {
      "id": "d2f674cf-3716-40f6-bc2a-7c05b4c355bf",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "loading in other data as per other notebooks",
        "title": "loading in other data as per other notebooks"
      },
      "source": "Water_Quality_df = pd.read_csv(\"water_quality_training_dataset.csv\")\nlandsat_train_features = pd.read_csv(\"landsat_features_training_all_bands.csv\")\nTerraclimate_df = pd.read_csv(\"terraclimate_features_training.csv\")\n\n##landsat_train_features['NDMI'] = landsat_train_features['NDMI'].astype(float)\n##landsat_train_features['MNDWI'] = landsat_train_features['MNDWI'].astype(float)\nlandsat_train_features['Sample Date'] = pd.to_datetime(landsat_train_features['Sample Date'],  format='%d-%m-%Y')\n\ndef combine_two_datasets(dataset1,dataset2,dataset3):\n    '''\n    Returns a  vertically concatenated dataset.\n    Attributes:\n    dataset1 - Dataset 1 to be combined \n    dataset2 - Dataset 2 to be combined\n    '''\n    \n    data = pd.concat([dataset1,dataset2,dataset3], axis=1)\n    data = data.loc[:, ~data.columns.duplicated()]\n    return data\n\nwq_data = combine_two_datasets(Water_Quality_df, landsat_train_features, Terraclimate_df)\nwq_data['Sample Date'] = pd.to_datetime(wq_data['Sample Date'],  format='%d-%m-%Y')\nwq_data = wq_data.drop(columns=[\"Unnamed: 0\"])\n\nfor column in wq_data.columns:\n    wq_data[wq_data[column] == -9999][column] = np.nan \n\n\ndisplay(wq_data.head(5))\n\nWater_Quality_df_validate = pd.read_csv(\"submission_template.csv\")\nlandsat_train_validate = pd.read_csv(\"landsat_features_validation.csv\")\nTerraclimate_df_validate = pd.read_csv(\"terraclimate_features_validation.csv\")\n\nlandsat_train_validate['NDMI'] = landsat_train_validate['NDMI'].astype(float)\nlandsat_train_validate['MNDWI'] = landsat_train_validate['MNDWI'].astype(float)\nlandsat_train_validate['Sample Date'] = pd.to_datetime(landsat_train_validate['Sample Date'],  format='%d-%m-%Y')\n\nwq_data_validation = combine_two_datasets(Water_Quality_df_validate, landsat_train_validate, Terraclimate_df_validate)\nwq_data_validation['Sample Date'] = pd.to_datetime(wq_data_validation['Sample Date'],  format='%d-%m-%Y')",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "12cb8273-9171-4d6d-8544-0c6298cf44bc",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Initial Exploration:\n\nBegin by just observing the Metadata"
    },
    {
      "id": "a7fa83d7-6ea5-4b72-8c31-db1c44bffd49",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "shape",
        "title": "shape"
      },
      "source": "wq_data.shape",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e8e5ee50-7ca9-4f03-b175-348a6de4b65c",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "metadata",
        "title": "metadata"
      },
      "source": "wq_data.info()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6318fec4-c127-4796-976a-243e5e919308",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Approx ~ 1100 null values are in the dataaet. These are pretty common in the training set and also present in the validation and submission datasets - so we cannot ignore"
    },
    {
      "id": "bed7c3a2-5940-49a6-b631-10eae571e8c0",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Single Variate Distributions:\n\nPlotting the histogram all the variables to see their distribution. This is useful to understand the qualities of the dataset, and may help in choosing a algorithm to fit against the data. \n\nMore of note, distribution analysis comes in handy as we need to do some Imputation of missing data values in Landsat features."
    },
    {
      "id": "490bc024-50d7-4306-96a9-972af0152367",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "summary stats",
        "title": "summary stats"
      },
      "source": "wq_data_EDA = wq_data.drop(columns=['Latitude', 'Longitude'])\n\nwq_data_EDA.describe().T",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e7572370-16ce-46d7-a476-2d17bbfccac0",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "histogram plots",
        "title": "histogram plots"
      },
      "source": "numerical_columns = wq_data_EDA.select_dtypes(include=[\"int64\", \"float64\"]).columns\n\nsns.set_palette(\"Blues\")\n\nplt.figure(figsize=(100, len(numerical_columns) * 5))\nfor idx, feature in enumerate(numerical_columns, 1):\n    plt.subplot(len(numerical_columns), 2, idx)\n    sns.histplot(wq_data_EDA[feature], kde=True)\n    plt.title(f\"{feature} | Skewness: {round(wq_data_EDA[feature].skew(), 2)}\")\n\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dd95103c-f8e7-4d96-ba4d-5d420d429ecd",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Lets ignore Lat and Long as they aren't really that relevant for EDA.\n\n - Most of our LandSat Predictors are unimodally distributed (dare I say almost normal?) albeit with very large positive skews. Any model we fit may need some checks against positive skewed values as they could be high leverage observations. The only outlier to this is pet, which seems to be normally distributed.\n\nOur dependent variables are distributed very interestingly. All three share the qualities of being multimodal, although Alkalinity could be argued that it is one stretched mode long mode. Important to note: all three response variables have a really long tail. This can really increase SE estimates and have a large impact on model fitting. \n\n### Transformations and Outlier Handling\n\nWe might transform these variables so that we can fit them better. Plotting the Box-Cox transformation gives:\n\nAlso replace the response variable in the dataframe with the box cox version."
    },
    {
      "id": "396e68d1-2f53-44f3-8d4e-7e9be2c50a11",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Box Cox transformation of response",
        "title": "Box Cox transformation of response"
      },
      "source": "Total_Alkalinity_bc, Total_Alkalinity_lambda_opt = stats.boxcox(wq_data_EDA['Total Alkalinity'])\nElectrical_Conductance_bc, Electrical_Conductance_lambda_opt = stats.boxcox(wq_data_EDA['Electrical Conductance'])\nDissolved_Reactive_Phosphorus_bc, Dissolved_Reactive_Phosphorus_lambda_opt = stats.boxcox(wq_data_EDA['Dissolved Reactive Phosphorus'])\n\nprint(f\"Total Alkalinity Optimal Lambda (λ): {Total_Alkalinity_lambda_opt}\")\nprint(f\"Electrical Conductance Optimal Lambda (λ): {Electrical_Conductance_lambda_opt}\")\nprint(f\"Dissolved Reactive Phosphorus Optimal Lambda (λ): {Dissolved_Reactive_Phosphorus_lambda_opt}\")\n\nplt.figure(figsize=(20, 7))\n\nplt.subplot(1, 3, 1)\nplt.hist(Total_Alkalinity_bc, bins=30, color='blue', alpha=0.7)\nplt.title('Total Alkalinity Transformed Data (Box-Cox)')\n\nplt.subplot(1, 3, 2)\nplt.hist(Electrical_Conductance_bc, bins=30, color='green', alpha=0.7)\nplt.title('Electrical Conductance Transformed Data (Box-Cox)')\n\nplt.subplot(1, 3, 3)\nplt.hist(Dissolved_Reactive_Phosphorus_bc, bins=30, color='green', alpha=0.7)\nplt.title('Dissolved Reactive Phosphorus Transformed Data (Box-Cox)')\n\nplt.show()\n\n##wq_data_EDA['Total Alkalinity'] = Total_Alkalinity_bc\n##wq_data_EDA['Electrical Conductance'] = Electrical_Conductance_bc\n##wq_data_EDA['Dissolved Reactive Phosphorus'] = Dissolved_Reactive_Phosphorus_bc",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dd53e877-05b8-4779-8632-6b55f9f3499c",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "This could help our analysis\n\nConsider lastly just a check for outlieres with a boxplot on the predictor variables that have high skew. As their leverage will ruin our analysis"
    },
    {
      "id": "9f3790fe-c226-4002-b1b4-b04c7afe8ff1",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Boxplot skewed predictors",
        "title": "Boxplot skewed predictors"
      },
      "source": "##boxplot = wq_data_EDA.boxplot(column=[\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"])\nboxplot = wq_data_EDA.boxplot(column=[\"nir\", \"green\", \"swir16\", \"swir22\"])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c123bab9-bdc1-4473-8ad4-b8b4701f1686",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "The existence of some outliers is concerning, we may also want to conduct some transformation on the predictors. But first lets just see what the validation dataset looks like in terms of these predictors:"
    },
    {
      "id": "910f9794-82fd-48d0-9976-2a9fd6cfec97",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Check validation set for outliers and skew",
        "title": "Check validation set for outliers and skew"
      },
      "source": "numerical_columns = wq_data_validation.select_dtypes(include=[\"int64\", \"float64\"]).columns\n\nsns.set_palette(\"Reds\")\n\nplt.figure(figsize=(20, len(numerical_columns) * 2))\nfor idx, feature in enumerate(numerical_columns, 1):\n    plt.subplot(len(numerical_columns), 2, idx)\n    sns.histplot(wq_data_validation[feature], kde=True)\n    plt.title(f\"{feature} | Skewness: {round(wq_data_validation[feature].skew(), 2)}\")\n\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1b08f11a-47e2-451d-b853-139519a6189f",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "The submission dataset doesn't have this skew, we could be cheeky and remove the 'outliers' from the training set. For nir, green and swir, this maxes out at 20,000 compared to the 60,000 in the training. If you refer to the boxplot above, you'll see it fits the box and whisker plot nicely.\n\nBut reducing these nir, green and swir figures will also reduce the NDMI and MNDWI, as they are a ratio of above. Comparing the two histograms of training vs validation data will indicate that the skew in these variables is reduced as well.\n\nWe will do some explorations of this in the next section."
    },
    {
      "id": "f7ed1ea4-402d-401b-abae-4e61f55c5045",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Multi Variate Analysis\n\nNOTE: WE MAY NEED TO SEGMENT OUT THE LANDSAT AND TERRACLIMATE, PENDING ON WHETHER OR NOT WE CAN EXTRACT AND MAP THE TERRACLIMATE\n\nLets now look at how the variates interact with each other to start looking at the underpinnings of a model.\n\nFirst a pairwise plot against the transformed predictors to see how all the variables are related."
    },
    {
      "id": "2eae9e25-d460-4a44-942b-6f4bae212887",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Pairwise on the Original",
        "title": "Pairwise on the Original"
      },
      "source": "sns.set_palette(\"Blues\")\n\nplt.figure(figsize=(25, 25))\n\nsns.pairplot(wq_data_EDA)\n\nplt.suptitle('Pair Plot for DataFrame')\nplt.show()\n\nwq_data_EDA_nodate = wq_data_EDA.drop(columns=['Sample Date'])\n\nplt.figure(figsize=(20, 20))\n\nsns.heatmap(wq_data_EDA_nodate.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)\n\nplt.title('Correlation Heatmap')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "879ed3d4-cb96-42d7-a69d-b74572fc3b44",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "There are a couple things of importance to note here:\n - Predictors nir, green, swir 16 and swir 22 are highly correlated. Fitting more than 1 or 2 may result in an overfit of the model. We can consider some dimension reduction of these predictors as well.\n - Pairwise plots of the three response variables indicate little correlation. However this could be to do with scaling issues and outliers. A transformation to reduce extreme values (or the removal of such) may actually help the correlation to appear!\n - PET is not correlated with any of the landsat data\n\nFurther, some of other steps we can take. We can consider the Box-Cox transformation of the response above and then re-consider the pairwise plots:\n"
    },
    {
      "id": "260f6477-0ef5-4c53-ad57-f2b0b2e1679e",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Pairwise on Box Cox",
        "title": "Pairwise on Box Cox"
      },
      "source": "wq_data_EDA['Total Alkalinity'] = Total_Alkalinity_bc\nwq_data_EDA['Electrical Conductance'] = Electrical_Conductance_bc\nwq_data_EDA['Dissolved Reactive Phosphorus'] = Dissolved_Reactive_Phosphorus_bc\n\nwq_data_EDA_nodate = wq_data_EDA.drop(columns=['Sample Date'])\n\nplt.figure(figsize=(25, 25))\n\nsns.pairplot(wq_data_EDA)\n\nplt.suptitle('Pair Plot for DataFrame')\nplt.show()\n\nwq_data_EDA_nodate = wq_data_EDA.drop(columns=['Sample Date'])\n\nplt.figure(figsize=(20, 20))\n\nsns.heatmap(wq_data_EDA_nodate.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)\n\nplt.title('Correlation Heatmap')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b42a2f67-647c-4ba9-a728-8fb38a479657",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "It still doesn't look great. The three response variables are now much more correlated with each other. Seems an approach we can take is try and use one or another of the response variables to predict the others, once we have an estimate.\n\nOtherwise it is still hard to determine any relationship between the predictors and the response. Partly this is because of the skewness in the predictors, we can try one of two things for our EDA.\n\nFirstly lets consider removing some of the outliers. \n\n### Predictor Outlier and Boundary Analysis\n\nLets just start by plotting the four main landsat features against themselves. This is the training dataset"
    },
    {
      "id": "879c4ab8-cf14-4c1a-8171-ed1a5927edc5",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Checking the Pairwise on the Predictors",
        "title": "Checking the Pairwise on the Predictors"
      },
      "source": "wq_data_EDA_ladnsat_predictors = wq_data_EDA[[\"nir\", \"green\", \"swir16\", \"swir22\"]]\n\nplt.figure(figsize=(25, 25))\n\nsns.pairplot(wq_data_EDA_ladnsat_predictors)\n\nplt.suptitle('Pair Plot for DataFrame')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0612475c-bbc3-45b4-b20c-142bf2b4c190",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "We note that alot of the ourliers for each nir, green, swir 16, swir 22 are also outliers for the other predictors, so it may just be a select few datapoints we are removing. \n\nAgain for the validation dataset - predictors pairwise gives:"
    },
    {
      "id": "74527a5b-281f-402b-be3d-f084fc2eb9de",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Pairwise Predictors for Validation",
        "title": "Pairwise Predictors for Validation"
      },
      "source": "wq_data_validation_ladnsat_predictors = wq_data_validation[[\"nir\", \"green\", \"swir16\", \"swir22\"]]\n\nsns.set_palette(\"Reds\")\n\nplt.figure(figsize=(25, 25))\n\nsns.pairplot(wq_data_validation_ladnsat_predictors)\n\nplt.suptitle('Pair Plot for DataFrame')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5290910d-20b9-42f7-b3f1-6f41481753c2",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "Observe a less linear relationship visible in the validation set. Also note the bounds of each predictor in the validation. \n\nIt makes sense to train the data on a set that indicative of the overall validation set. Lets observe what happens when we do som restricting.\n\n### Predictor Outlier Removal\n\nLets remove some of the outlier values from the training set and re-check the pairwise plots"
    },
    {
      "id": "97947003-2538-435e-b270-3bc1bc1b81d8",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Pairwise on Training with outliers removed",
        "title": "Pairwise on Training with outliers removed"
      },
      "source": "wq_data_EDA_nodate_filtered = wq_data_EDA_nodate[wq_data_EDA_nodate[\"green\"] < 20000]\nwq_data_EDA_nodate_filtered = wq_data_EDA_nodate_filtered[wq_data_EDA_nodate_filtered[\"swir16\"] < 20000]\nwq_data_EDA_nodate_filtered = wq_data_EDA_nodate_filtered[wq_data_EDA_nodate_filtered[\"swir22\"] < 18000]\n##wq_data_EDA_nodate_filtered = wq_data_EDA_nodate_filtered[wq_data_EDA_nodate_filtered[\"nir\"] < 25000]\n\n\nsns.set_palette(\"Blues\")\n\nplt.figure(figsize=(25, 25))\n\nsns.pairplot(wq_data_EDA_nodate_filtered, kind='reg')\n\nplt.suptitle('Pair Plot for DataFrame')\nplt.show()\n\nplt.figure(figsize=(20, 20))\n\nsns.heatmap(wq_data_EDA_nodate_filtered.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)\n\nplt.title('Correlation Heatmap')\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "deac0bff-1741-41b4-87fa-374406b5fcb8",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "This still doesn't give us much unfortunately, but it does give us a few things:\n - Correlation between Alkalinity and Electrical Conductance does exist. So if we can fit one, another can also be fitted\n  - 'Green' is looking like an indicator variable, with two distinct groups. This can be seen in both the test and training data (<7500, >7500) with a difference in means.\n  - PET is strong, perhaps we can fit more terradata as well.\n  - At this point, NIR + Green are probably the two others to consider?\n\n### Other things to Consider:\nThe following can also be considered as part of the analysis:\n- Maybe can try transformations of the variables (e.g. functions, indicator variables)\n- Lag or Lead of variables could help as well\n- Time series analysis"
    },
    {
      "id": "b3e47e9d-a379-478a-a924-3a4dbca9c76a",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Analysis of Null values\n\nBegin by just comparing where the null values take place, both spacially and temporally:\n\n "
    },
    {
      "id": "272099c3-f1bc-47a8-8aff-11d0c55ef60b",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Assessment of Null Values in Spatial Analysis",
        "title": "Assessment of Null Values in Spatial Analysis"
      },
      "source": "wq_data_nonullvalues = wq_data.dropna(how='any',axis=0)\nwq_data_nullvalues = wq_data[wq_data[\"nir\"].isnull()]\n\nwq_data_nonullvalues_validation = wq_data_validation[~wq_data_validation[\"nir\"].isnull()]\nwq_data_nullvalues_validation = wq_data_validation[wq_data_validation[\"nir\"].isnull()]\n\nplt.scatter(wq_data_nonullvalues[\"Latitude\"], wq_data_nonullvalues[\"Longitude\"] , color='blue', label='Non-Null - Training', marker='.')\n\nplt.scatter(wq_data_nullvalues[\"Latitude\"], wq_data_nullvalues[\"Longitude\"] , color='blue', label='Null - Training', marker='x')\n\nplt.scatter(wq_data_nonullvalues_validation[\"Latitude\"], wq_data_nonullvalues_validation[\"Longitude\"] , color='red', label='Non-Null - Validation', marker='.')\n\nplt.scatter(wq_data_nullvalues_validation[\"Latitude\"], wq_data_nullvalues_validation[\"Longitude\"] , color='red', label='Null - Validation', marker='x')\n\n# Add labels, a title, and a legend\nplt.xlabel('Latitude')\nplt.ylabel('Longitude')\nplt.title('Assessment of Null Values by Lat and Long')\nplt.legend() # Displays the labels we set in plt.scatter()\n\n# Display the plot\nplt.show()\n\n# Plotting\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=wq_data_nonullvalues['Sample Date'], fill=False, label='Training - No Null', common_norm=False, color = \"Blue\")\nsns.kdeplot(data=wq_data_nullvalues['Sample Date'], fill=False, label='Training - Null', common_norm=False, color = \"Blue\", ls='--')\nsns.kdeplot(data=wq_data_nonullvalues_validation['Sample Date'], fill=False, label='Validation - No Null', common_norm=False, color = \"Red\")\nsns.kdeplot(data=wq_data_nullvalues_validation['Sample Date'], fill=False, label='Validation - Null', common_norm=False, color = \"Red\", ls='--')\nplt.title('Overlayed KDE Plots (Different Sizes)')\nplt.legend()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ca10ff02-8b3f-4a25-89b4-8b4369c61c86",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "It seems all the nuull values occur in the same place, although their time frames might be different (observe the kernel density plot). Lets confirm exactly how many null value we have exact matches for:"
    },
    {
      "id": "fe67f017-2342-469e-8d0d-74acefbad51e",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Checking Exact Matches",
        "title": "Checking Exact Matches"
      },
      "source": "merged_no_null = pd.concat([wq_data_nonullvalues, wq_data_nonullvalues_validation])\nmerged_no_null = merged_no_null[[\"Latitude\", \"Longitude\", \"Sample Date\"]]\n\nno_null_training_check = pd.merge(wq_data_nullvalues, merged_no_null, on=[\"Latitude\", \"Longitude\", \"Sample Date\"], how='inner')\nno_null_validation_check = pd.merge(wq_data_nullvalues_validation, merged_no_null, on=[\"Latitude\", \"Longitude\", \"Sample Date\"], how='inner')\n\nprint(wq_data_nullvalues.shape)\nprint(no_null_training_check.shape)\n\nprint(wq_data_nullvalues_validation.shape)\nprint(no_null_validation_check.shape)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e98b8f48-79e5-4400-9b23-a92ed9960759",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "So it seems while the locations are the same, the dates are different. We will need to impute missing values\n\nThe good news is the data seems missing at random and so imputation is possible!"
    },
    {
      "id": "1335079e-e17b-4ce4-ad0a-bdb6b42b777f",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Next Steps\n\nNull value handling!\nFile at:"
    }
  ]
}
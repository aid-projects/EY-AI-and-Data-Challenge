{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "afd0ad9f-c4a5-453e-a40e-cb0a1f243804",
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# !pip install uv\n",
    "# !uv pip install  -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T00:41:02.405456Z",
     "start_time": "2026-02-21T00:41:02.395423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import snowflake\n",
    "# from snowflake.snowpark.context import get_active_session\n",
    "#\n",
    "# session = get_active_session()"
   ],
   "id": "aec004d838247006",
   "outputs": [],
   "execution_count": 1
  },
  {
   "id": "f84f8bc3-07d7-45f1-9db5-4de285eb345f",
   "cell_type": "code",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-21T00:41:03.957767Z",
     "start_time": "2026-02-21T00:41:03.951243Z"
    }
   },
   "source": [
    "# import warnings\n",
    "# from benchmark_model_notebook_snowflake import landsat_val_features\n",
    "#\n",
    "# warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "95040bbf-1848-45f0-aab4-1d8ba434f934",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# # Stage or location to check\n",
    "# stage_location = \"\"\"snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/terraclimate_training_soil_data/\"\"\"\n",
    "#\n",
    "# # Run LIST command to get files in the stage\n",
    "# df_files = session.sql(f\"LIST '{stage_location}'\")\n",
    "#\n",
    "# # Count the number of files\n",
    "# file_count = df_files.count()\n",
    "#\n",
    "# df_files.show()\n",
    "#\n",
    "# print(f\"Number of files in {stage_location}: {file_count}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "ce63f2e7-97a0-4627-9e90-c0e27a0d29a3",
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# from pathlib import Path\n",
    "# import zipfile\n",
    "#\n",
    "# folder_name = \"terraclimate_training_vpd_data\"\n",
    "#\n",
    "# source_dir = Path(f\"./{folder_name}/\")      # folder containing 60 CSVs\n",
    "# zip_path   = Path(f\"/tmp/{folder_name}.zip\")\n",
    "#\n",
    "# csv_files = sorted(source_dir.glob(\"*.csv\"))  # or \"**/*.csv\" for recursive\n",
    "#\n",
    "# if not csv_files:\n",
    "#     raise FileNotFoundError(f\"No CSV files found in {source_dir}\")\n",
    "#\n",
    "# with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "#     for file in csv_files:\n",
    "#         zf.write(file, arcname=file.name)\n",
    "#\n",
    "# print(f\"Created: {zip_path} ({len(csv_files)} files)\")\n",
    "#\n",
    "# session.sql(f\"\"\"\n",
    "#     PUT file:///tmp/{folder_name}.zip\n",
    "#     'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n",
    "#     AUTO_COMPRESS=FALSE\n",
    "#     OVERWRITE=TRUE\n",
    "# \"\"\").collect()\n",
    "#\n",
    "# print(\"File saved! Refresh the browser to see the files in the sidebar\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T00:57:09.804793Z",
     "start_time": "2026-02-21T00:57:09.555074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from attr import dataclass"
   ],
   "id": "e0f89277c2d830dd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T00:57:09.810100Z",
     "start_time": "2026-02-21T00:57:09.807380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# User Entered Parameters\n",
    "INCLUDE_RAW_BANDS = True\n",
    "DATA_DIR = \"data\"\n",
    "SOURCE_DATA_DIRS = ['landsat']"
   ],
   "id": "277af5ce6947eed5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "id": "6222361a-4508-4158-8b43-331eac2384a2",
   "cell_type": "code",
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-21T02:15:09.555337Z",
     "start_time": "2026-02-21T02:15:09.542961Z"
    }
   },
   "source": [
    "landsat_variables = ['NDMI', 'MNDWI'] # Baseline\n",
    "landsat_variables_mvdb = ['EVI', 'OSAVI', 'GNDVI', 'GCVI', 'MSI', 'NBR', 'Green/Red Ratio', 'NDGI', 'UI (Urban Index)', 'NBR2', 'Red/NIR Ratio', 'Green/NIR Ratio', 'NDWI']\n",
    "landsat_variables_ross = ['NVDI', 'SAVI', 'BSI', 'NDBI', 'TCWI', ]\n",
    "\n",
    "landsat_variables.extend(landsat_variables_mvdb)\n",
    "landsat_variables.extend(landsat_variables_ross)\n",
    "landsat_variables = [var.lower() for var in landsat_variables]\n",
    "\n",
    "bands_of_interest = ['qa', 'red', 'blue', 'drad', 'emis', 'emsd', 'lwir', 'trad', 'urad', 'atran', 'cdist', 'green', 'nir08', 'lwir', 'swir16', 'swir22', 'cloud_qa', 'qa_pixel', 'qa_radsat', 'atmos_opacity']\n",
    "if INCLUDE_RAW_BANDS:\n",
    "    landsat_variables.extend(bands_of_interest)"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:30:21.371694Z",
     "start_time": "2026-02-21T02:30:21.355301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_dataframe_cols_lowercase(df):\n",
    "    for col in df.columns:\n",
    "        df.rename(columns={col: col.lower()}, inplace=True)\n",
    "\n",
    "def add_formatted_join_column(df, join_col_name, drop_cols=None):\n",
    "    df['sample date'] = pd.to_datetime(df['sample date'], format='mixed').dt.strftime('%d-%m-%Y')\n",
    "    df['latitude'] = df['latitude'].round(6)\n",
    "    df['longitude'] = df['longitude'].round(6)\n",
    "    df[join_col_name] = df['latitude'].astype(str) + \"~\" + df['longitude'].astype(str) + \"~\" + df['sample date']\n",
    "    if drop_cols:\n",
    "        df.drop(columns=drop_cols, inplace=True)\n"
   ],
   "id": "e0ed68f2fe7b9fdd",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:39:03.973469Z",
     "start_time": "2026-02-21T02:39:03.682552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dfs = {}\n",
    "submission_dfs = {}\n",
    "\n",
    "for i in range(len(SOURCE_DATA_DIRS)):\n",
    "    current_dir = SOURCE_DATA_DIRS[i]\n",
    "    source_dir = Path(f\"./{DATA_DIR}/{current_dir}/\")\n",
    "    print(f\"Processing data in: {source_dir}...\")\n",
    "\n",
    "    csv_files = sorted(source_dir.glob(\"*.csv\"))\n",
    "    training_df = pd.read_csv(f'./{DATA_DIR}/water_quality_training_dataset.csv')\n",
    "    submission_df = pd.read_csv(f'./{DATA_DIR}/submission_template.csv')\n",
    "    training_df_row_num = training_df.shape[0]\n",
    "    submission_df_row_num = submission_df.shape[0]\n",
    "\n",
    "    # Ensure columns are all lowercase\n",
    "    make_dataframe_cols_lowercase(training_df)\n",
    "    make_dataframe_cols_lowercase(submission_df)\n",
    "\n",
    "    add_formatted_join_column(training_df, \"join_column\")\n",
    "    add_formatted_join_column(submission_df, \"join_column\")\n",
    "\n",
    "    join_columns = ['latitude', 'longitude', 'sample date']\n",
    "\n",
    "    for i in range(len(csv_files)):\n",
    "\n",
    "        print(f\"\\tProcessing {csv_files[i]}...\")\n",
    "        data_df = pd.read_csv(f\"{csv_files[i]}\")\n",
    "        make_dataframe_cols_lowercase(data_df)\n",
    "\n",
    "\n",
    "        # Only keep columns we want to keep\n",
    "        keep_columns_train = [col for col in data_df.columns if col in landsat_variables and col not in training_df.columns]\n",
    "        keep_columns_val = [col for col in data_df.columns if col in landsat_variables and col not in submission_df.columns]\n",
    "        keep_columns_train = sorted(keep_columns_train)\n",
    "        keep_columns_val = sorted(keep_columns_val)\n",
    "        keep_columns_train.extend(join_columns)\n",
    "        keep_columns_val.extend(join_columns)\n",
    "\n",
    "        if 'training' in str(csv_files[i]):\n",
    "            data_df = data_df[keep_columns_train]\n",
    "            add_formatted_join_column(data_df, \"join_column\", drop_cols=join_columns)\n",
    "            training_df = training_df.merge(data_df, on=\"join_column\", how='inner')\n",
    "\n",
    "        elif 'validation' in str(csv_files[i]):\n",
    "            data_df = data_df[keep_columns_val]\n",
    "            add_formatted_join_column(data_df, \"join_column\", drop_cols=join_columns)\n",
    "            submission_df = submission_df.merge(data_df, on=\"join_column\", how='inner')\n",
    "\n",
    "    training_df.drop(columns=['join_column'], inplace=True)\n",
    "    submission_df.drop(columns=['join_column'], inplace=True)\n",
    "\n",
    "    assert training_df_row_num == training_df.shape[0], f\"{training_df_row_num - training_df.shape[0]} rows dropped from training_df!\"\n",
    "    assert submission_df_row_num == submission_df.shape[0], f\"{submission_df_row_num - submission_df.shape[0]} rows dropped from submission_df!\"\n",
    "\n",
    "    training_dfs[current_dir] = (training_df, submission_df)\n"
   ],
   "id": "8296ec061c26b181",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data in: data\\landsat...\n",
      "\tProcessing data\\landsat\\landsat_features_training_all_bands.csv...\n",
      "\tProcessing data\\landsat\\landsat_features_training_baseline.csv...\n",
      "\tProcessing data\\landsat\\landsat_features_training_mvdb.csv...\n",
      "\tProcessing data\\landsat\\landsat_features_training_ross.csv...\n",
      "\tProcessing data\\landsat\\landsat_features_validation_baseline.csv...\n",
      "\tProcessing data\\landsat\\landsat_features_validation_ross.csv...\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:39:04.718353Z",
     "start_time": "2026-02-21T02:39:04.715256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_df = training_dfs['landsat'][0]\n",
    "validation_df = training_dfs['landsat'][1]"
   ],
   "id": "f01255e92707c17d",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T02:39:05.134582Z",
     "start_time": "2026-02-21T02:39:05.009104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Landsat Training data\n",
    "training_df.to_csv(\"./data/landsat_features_training_combined.csv\", index=False)"
   ],
   "id": "d322f76c8a5e131",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "language": "python",
    "ExecuteTime": {
     "end_time": "2026-02-21T02:39:05.286879Z",
     "start_time": "2026-02-21T02:39:05.277566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Landsat Validation data\n",
    "validation_df.to_csv(\"./data/landsat_features_validation_combined.csv\", index=False)"
   ],
   "id": "6b421d8f-fac8-4efd-9152-538711701230",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e3490f625da590c7"
  }
 ]
}

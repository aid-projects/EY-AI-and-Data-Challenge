{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "id": "afd0ad9f-c4a5-453e-a40e-cb0a1f243804",
      "metadata": {
        "language": "python"
      },
      "source": "# !pip install uv\n# !uv pip install  -r requirements.txt",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "aec004d838247006",
      "metadata": {
        "language": "python"
      },
      "source": " import snowflake\n from snowflake.snowpark.context import get_active_session\n\n session = get_active_session()",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "id": "f84f8bc3-07d7-45f1-9db5-4de285eb345f",
      "metadata": {
        "language": "python"
      },
      "source": "# import warnings\n# from benchmark_model_notebook_snowflake import landsat_val_features\n#\n# warnings.filterwarnings(\"ignore\")",
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "id": "95040bbf-1848-45f0-aab4-1d8ba434f934",
      "metadata": {
        "language": "python"
      },
      "source": "# # Stage or location to check\n# stage_location = \"\"\"snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/terraclimate_training_soil_data/\"\"\"\n#\n# # Run LIST command to get files in the stage\n# df_files = session.sql(f\"LIST '{stage_location}'\")\n#\n# # Count the number of files\n# file_count = df_files.count()\n#\n# df_files.show()\n#\n# print(f\"Number of files in {stage_location}: {file_count}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "ce63f2e7-97a0-4627-9e90-c0e27a0d29a3",
      "metadata": {
        "language": "python"
      },
      "source": "# from pathlib import Path\n# import zipfile\n#\n# folder_name = \"terraclimate_training_vpd_data\"\n#\n# source_dir = Path(f\"./{folder_name}/\")      # folder containing 60 CSVs\n# zip_path   = Path(f\"/tmp/{folder_name}.zip\")\n#\n# csv_files = sorted(source_dir.glob(\"*.csv\"))  # or \"**/*.csv\" for recursive\n#\n# if not csv_files:\n#     raise FileNotFoundError(f\"No CSV files found in {source_dir}\")\n#\n# with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n#     for file in csv_files:\n#         zf.write(file, arcname=file.name)\n#\n# print(f\"Created: {zip_path} ({len(csv_files)} files)\")\n#\n# session.sql(f\"\"\"\n#     PUT file:///tmp/{folder_name}.zip\n#     'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n#     AUTO_COMPRESS=FALSE\n#     OVERWRITE=TRUE\n# \"\"\").collect()\n#\n# print(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e0f89277c2d830dd",
      "metadata": {
        "language": "python"
      },
      "source": "from pathlib import Path\nimport pandas as pd\nfrom attr import dataclass",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "id": "277af5ce6947eed5",
      "metadata": {
        "language": "python"
      },
      "source": "# User Entered Parameters\nINCLUDE_RAW_BANDS = True\nDATA_DIR = \"data\"\nSOURCE_DATA_DIRS = ['landsat']",
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "id": "6222361a-4508-4158-8b43-331eac2384a2",
      "metadata": {
        "language": "python"
      },
      "source": "landsat_variables = ['NDMI', 'MNDWI'] # Baseline\nlandsat_variables_mvdb = ['EVI', 'OSAVI', 'GNDVI', 'GCVI', 'MSI', 'NBR', 'Green/Red Ratio', 'NDGI', 'UI (Urban Index)', 'NBR2', 'Red/NIR Ratio', 'Green/NIR Ratio', 'NDWI']\nlandsat_variables_ross = ['NVDI', 'SAVI', 'BSI', 'NDBI', 'TCWI', ]\n\nlandsat_variables.extend(landsat_variables_mvdb)\nlandsat_variables.extend(landsat_variables_ross)\nlandsat_variables = [var.lower() for var in landsat_variables]\n\nbands_of_interest = ['qa', 'red', 'blue', 'drad', 'emis', 'emsd', 'lwir', 'trad', 'urad', 'atran', 'cdist', 'green', 'nir08', 'lwir', 'swir16', 'swir22', 'cloud_qa', 'qa_pixel', 'qa_radsat', 'atmos_opacity']\nif INCLUDE_RAW_BANDS:\n    landsat_variables.extend(bands_of_interest)",
      "outputs": [],
      "execution_count": 57
    },
    {
      "cell_type": "code",
      "id": "e0ed68f2fe7b9fdd",
      "metadata": {
        "language": "python"
      },
      "source": "def make_dataframe_cols_lowercase(df):\n    for col in df.columns:\n        df.rename(columns={col: col.lower()}, inplace=True)\n\ndef add_formatted_join_column(df, join_col_name, drop_cols=None):\n    df['sample date'] = pd.to_datetime(df['sample date'], format='mixed').dt.strftime('%d-%m-%Y')\n    df['latitude'] = df['latitude'].round(6)\n    df['longitude'] = df['longitude'].round(6)\n    df[join_col_name] = df['latitude'].astype(str) + \"~\" + df['longitude'].astype(str) + \"~\" + df['sample date']\n    if drop_cols:\n        df.drop(columns=drop_cols, inplace=True)",
      "outputs": [],
      "execution_count": 76
    },
    {
      "cell_type": "code",
      "id": "8296ec061c26b181",
      "metadata": {
        "language": "python"
      },
      "source": "training_dfs = {}\nsubmission_dfs = {}\n\nfor i in range(len(SOURCE_DATA_DIRS)):\n    current_dir = SOURCE_DATA_DIRS[i]\n    source_dir = Path(f\"./{DATA_DIR}/{current_dir}/\")\n    print(f\"Processing data in: {source_dir}...\")\n\n    csv_files = sorted(source_dir.glob(\"*.csv\"))\n    training_df = pd.read_csv(f'./{DATA_DIR}/water_quality_training_dataset.csv')\n    submission_df = pd.read_csv(f'./{DATA_DIR}/submission_template.csv')\n    training_df_row_num = training_df.shape[0]\n    submission_df_row_num = submission_df.shape[0]\n\n    # Ensure columns are all lowercase\n    make_dataframe_cols_lowercase(training_df)\n    make_dataframe_cols_lowercase(submission_df)\n\n    add_formatted_join_column(training_df, \"join_column\")\n    add_formatted_join_column(submission_df, \"join_column\")\n\n    join_columns = ['latitude', 'longitude', 'sample date']\n\n    for i in range(len(csv_files)):\n\n        print(f\"\\tProcessing {csv_files[i]}...\")\n        data_df = pd.read_csv(f\"{csv_files[i]}\")\n        make_dataframe_cols_lowercase(data_df)\n\n\n        # Only keep columns we want to keep\n        keep_columns_train = [col for col in data_df.columns if col in landsat_variables and col not in training_df.columns]\n        keep_columns_val = [col for col in data_df.columns if col in landsat_variables and col not in submission_df.columns]\n        keep_columns_train = sorted(keep_columns_train)\n        keep_columns_val = sorted(keep_columns_val)\n        keep_columns_train.extend(join_columns)\n        keep_columns_val.extend(join_columns)\n\n        if 'training' in str(csv_files[i]):\n            data_df = data_df[keep_columns_train]\n            add_formatted_join_column(data_df, \"join_column\", drop_cols=join_columns)\n            training_df = training_df.merge(data_df, on=\"join_column\", how='inner')\n\n        elif 'validation' in str(csv_files[i]):\n            data_df = data_df[keep_columns_val]\n            add_formatted_join_column(data_df, \"join_column\", drop_cols=join_columns)\n            submission_df = submission_df.merge(data_df, on=\"join_column\", how='inner')\n\n    training_df.drop(columns=['join_column'], inplace=True)\n    submission_df.drop(columns=['join_column'], inplace=True)\n\n    assert training_df_row_num == training_df.shape[0], f\"{training_df_row_num - training_df.shape[0]} rows dropped from training_df!\"\n    assert submission_df_row_num == submission_df.shape[0], f\"{submission_df_row_num - submission_df.shape[0]} rows dropped from submission_df!\"\n\n    training_dfs[current_dir] = (training_df, submission_df)",
      "outputs": [],
      "execution_count": 97
    },
    {
      "cell_type": "code",
      "id": "f01255e92707c17d",
      "metadata": {
        "language": "python"
      },
      "source": "training_df = training_dfs['landsat'][0]\nvalidation_df = training_dfs['landsat'][1]",
      "outputs": [],
      "execution_count": 98
    },
    {
      "cell_type": "code",
      "id": "d322f76c8a5e131",
      "metadata": {
        "language": "python"
      },
      "source": "# Landsat Training data\ntraining_df.to_csv(\"./data/landsat_features_training_combined.csv\", index=False)",
      "outputs": [],
      "execution_count": 99
    },
    {
      "cell_type": "code",
      "id": "6b421d8f-fac8-4efd-9152-538711701230",
      "metadata": {
        "language": "python"
      },
      "source": "# Landsat Validation data\nvalidation_df.to_csv(\"./data/landsat_features_validation_combined.csv\", index=False)",
      "outputs": [],
      "execution_count": 100
    },
    {
      "cell_type": "code",
      "id": "e3490f625da590c7",
      "metadata": {
        "language": "python"
      },
      "source": "session.sql(f\"\"\"\n PUT file://data/landsat_features_validation_combined.csv\n 'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge - RW Dev\"/versions/live/data/'\n AUTO_COMPRESS=FALSE\n OVERWRITE=TRUE\n\"\"\").collect()\n\nsession.sql(f\"\"\"\n PUT file://data/landsat_features_training_combined.csv\n 'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge - RW Dev\"/versions/live/data/'\n AUTO_COMPRESS=FALSE\n OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    }
  ]
}